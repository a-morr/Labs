\lab{Markov Chains}{Markov Chains}
\label{lab:Markov}
\objective{
A \emph{Markov chain} is a finite collection of states with specified probabilities for transitioning from one state to another.
They are characterized by the fact that the future behavior of the system depends only on its current state.
Markov chains have far ranging applications; in this lab, we create a Markov chain for generating random English sentences.}

% TODO: Changing column_sum = 1 to row_sum = 1 (transpose all transition matrices, ij^th entry --> probability of going from from state i to state j (instead of the other way around). Makes implementing everything easier.)

% TODO: Update test driver, solutions+spec on class implementation.

\section*{State Space Models} % ===============================================

Many systems can be described by a finite number of states.
% A Markov chain is a collection of states, together with the probabilities of moving from one state to another.
For example, a board game where players move around the board based on die rolls can be modeled by a Markov chain.
Each space represents a state, and a player is said to be in a state if their piece is currently on the corresponding space.
In this case, the probability of moving from one space to another only depends on the players current location: where the player was on a previous turn does not affect their current turn.

Finite Markov chains have an associated transition matrix that stores all the information about the chain.
The $(ij)^{th}$ entry of the matrix gives the probability of moving from state $j$ to state $i$.
Thus the columns of the transition matrix must sum to 1. %\footnote{This type of transition matrix is called \emph{column-stochastic}. Row stochastic matrices are also often used ti model Markov chains.}
% TODO: Note about column-stochastic v row-stochastic transition matrices.

Consider a very simple weather model where the probability of being hot or cold depends on the weather of the previous day.
If the probability that tomorrow is hot given that today is hot is 0.7, and the probability that tomorrow is cold given that today is cold is 0.4, then by assigning hot to the $0^{th}$ row and column, and cold to the $1^{st}$ row and column, this Markov chain has the following transition matrix:

\[W = \left[\begin{array}{cc}
0.7 & 0.6 \\
0.3 & 0.4 \end{array} \right]\] 

% TODO: insert a PICTURE of a this Markov chain!

If it is hot today, we examine the $0^{th}$ column of $W$.
There is a $70\%$ chance that tomorrow will be hot ($0^{th}$ row), and a $30\%$ chance that tomorrow will be cold ($1^{st}$ row).
Conversely, if it is cold today, there is a $60\%$ chance that tomorrow will be hot, and a $40\%$ chance that tomorrow will be cold.

\begin{problem} % Problem: stochasticity. 
Transition matrices for Markov chains are efficiently stored as NumPy arrays.
Write a function that accepts a dimension $n$ and returns the transition matrix for a random Markov chain with $n$ states.
\\
(Hint: use array broadcasting to avoid looping.)
\end{problem}

\subsection*{Simulating State Transitions} % ----------------------------------

In a general finite Markov chain, if we are in state $j$ then the $j^{th}$ column of the transition matrix gives the probabilities of moving to any other state $i$.
By definition, these probabilities sum to $1$.
Thus, the entries of each column partition the interval $[0, 1]$, and we can choose the next state to move to by generating a random number between $0$ and $1$.

Consider again the weather model example from the previous section.
Suppose that today is hot, and that we want to simulate tomorrow's weather.
The column that corresponds to ``hot'' in the transition matrix is $[0.7, 0.3]^T$.
If we generate a random number and it is smaller than $0.3$, then our simulation indicates that tomorrow will be cold.
Conversely, if the random number is between $0.3$ and $1$, then the simulation says that tomorrow will be hot.
The following code implements this idea.

\begin{lstlisting}
import numpy as np

def forecast():
	"""Forecast tomorrow's weather given that today is hot."""

	transition_matrix = np.array([[0.7, 0.6], [0.3, 0.4]])
	# Sample from the standard uniform distribution to choose a new state.
	if np.random.random() < transition_matrix[1,0]:
		print "Cold"
		return 1
	else:
		print "Hot"
		return 0
\end{lstlisting}

\begin{problem} % Problem: Forecasting over several days.
Modify \li{forecast()} so that it accepts a parameter \li{days} and runs a simulation of the weather for the number of days given.
Return a list containing the day-by-day weather predictions (0 for hot, 1 for cold).
Assume the first day is hot, but do not include the data from the first day in the list of predictions.
The resulting list should therefore have \li{days} entries.
\end{problem}

For Markov chains with very few states, the approach in \li{forecast()} is practical and the implementation is fairly simple.
However, small Markov chains are typically not very useful in applications.

\subsection*{Larger Chains} % -------------------------------------------------

The \li{forecast()} function makes one random draw from a \emph{uniform} distribution to simulate a state change.
For larger Markov chains, we draw from a \emph{multinomial} distribution.
A multinomial distribution is a multivariate generalization of the binomial distribution.
A single draw from a binomial distribution with parameter $p$ indicates successes or failure of a single experiment with probability $p$ of success.
The classic example is a coin flip, where the $p$ is the probability that the coin lands heads side up.
A single draw from a multinomial distribution with parameters $\left(p_1, p_2, ..., p_n \right)$ indicates which of $n$ outcomes occurs.
In this case the classic example is a dice roll, with $6$ possible outcomes instead of the $2$ in a coin toss.

\begin{lstlisting}
# To simulate a single dice roll, store the probabilities of each outcome.
>>> probabilities = np.array([1./6, 1./6, 1./6, 1./6, 1./6, 1./6])

# Make a single random draw (roll the die once).
>>> np.random.multinomial(1, probabilities)         
array([0, 0, 0, 1, 0, 0])                       # The roll resulted in a 4.
\end{lstlisting}

\begin{problem} % Problem: 4 states instead of 2. Multinomial transitioning.
Let the following be the transition chain for a Markov chain modeling weather with four states: hot, mild, cold, and freezing.

\[ W^\prime = \left[\begin{array}{cccc}
0.5 & 0.3 & 0.1 & 0\\
0.3 & 0.3 & 0.3 & 0.3\\
0.2 & 0.3 & 0.4 & 0.5\\
  0 & 0.1 & 0.2 & 0.2\end{array} \right]\]
with hot, mild, cold, and freezing corresponding to columns (and rows) 0, 1, 2, and 3, respectively.

Write a new function that accepts a parameter \li{days} and runs the same kind of simulation as \li{forecast()}, but that uses the new four-state transition matrix.
This time, assume the first day is mild.
Return a list containing the day-to-day results (0 for hot, 1 for mild, 2 for cold, and 3 for freezing).
\label{problem:transition}
\end{problem}

\begin{problem} % Problem: Analysis of results.
Write a function that investigates and interprets the results of the simulations in the previous two problems.
Specifically, find the average percentage of days that are hot, mild, cold, and freezing in each simulation.
Does changing the starting day alter the results?
Print a report of your findings.
\end{problem}

\section*{Using Markov Chains to Simulate English} % ==========================
% TODO: is it okay to make this reference?
One of the original applications of Markov chains was to study natural languages.\footnote{In computer science, a \emph{natural language} is a spoken language, like English or Russian. See \url{http://langvillea.people.cofc.edu/MCapps7.pdf} for some details on the early applications of Markov chains, including the study of natural languages.}
In the early $20^{th}$ century, Markov used his chains to model how Russian switched from vowels to consonants.
By mid-century, they had been used as an attempt to model English.
It turns out that Markov chains are, by themselves, insufficient to model very good English.
However, they can approach a fairly good model of bad English, with sometimes amusing results.

By nature, a Markov chain is only concerned with its current state.
Thus a Markov chain simulating transitions between English words is completely unaware of context or even of previous words in a sentence.
For example, a Markov chain's current state may be the word ``continuous.''
Then the chain would say that the next word in the sentence is more likely to be ``function'' rather than ``raccoon.''
However, without the context of the rest of the sentence, even two likely words stringed together may result in gibberish.

We restrict ourselves to a subproblem of modeling the English of a specific file.
The transition probabilities of the resulting Markov chain will reflect the sort of English that the source authors speak.
Thus the Markov chain built from \emph{The Complete Works of William Shakespeare} will differ greatly from, say, the Markov chain built from a collection of academic journals.
We will call the source collection of works in the next problems the \emph{training set}.

\subsection*{Making the Chain} % ----------------------------------------------

In the weather models of the previous section, we chose a fixed number of states to simulate.
However, in English, sentences are of varying length.
To capture this feature in a Markov model, we include a \emph{start state} and an \emph{end state} in the model in addition to the words of the training set.
The start state should only transition to words that appear at the beginning of a sentence in the training set, and only words that appear at the end a sentence in the training set should transition to the end state.
Thus if a training set has $N$ unique words, the transition matrix will be $(N+2) \times (N+2)$.

After determining the states in the Markov chain, we need to determine the transition probabilities between the states.
As with the weather models, each state must be assigned a row and column in the transition matrix.
Then, while iterating through the sentences of the training set, we fill in the entries of the transition matrix.
When word $b$ follows word $a$, we add one to the $(b,a)^{th}$ entry of the matrix.
Once we have done this for every adjacent pair of words in the training set, we normalize the columns so that they each sum to one.

Consider the following small training set as an example.

\begin{lstlisting}
<<I am Sam Sam I am.
Do you like green eggs and ham?
I do not like them, Sam I am.
I do not like green eggs and ham.>>
\end{lstlisting}

If we include punctuation (so ``ham?'' and ``ham.'' are counted as distinct words) and do not alter the capitalization (so ``Do'' and ``do'' are also different), there are 15 unique words in this training set:
\begin{align*}
\text{``I'', ``am'', ``Sam'', ``am.'', ``Do'', ``you'', ``like'', ``green'',}
\\
\text{``eggs'', ``and'', ``ham?'', ``do'', ``not'', ``them,'', and ``ham.''.}
\end{align*}

With start and stop states, the transition matrix should therefore be $17 \times 17$.
The start state should transition to the words ``I'' and ``Do'', and the words ``am.'', ``ham?'', and ``ham?'' should transition to the end state.
Letting ``{\color[rgb]{.3,.6,.1}\$tart}'' represent the start state and ``{\color{red}\$top}'' represent the stop state, we first count the number of times that each state transitions to another state:

\begin{align*}
\begin{blockarray}{cccccccc}
& \text{\textcolor[rgb]{.3,.6,.1}{\$tart}} & \text{I} & \text{am} & \text{Sam} & & \text{ham.} & \text{\textcolor[rgb]{1,0,0}{\$top}} \\
\begin{block}{c(ccccccc)} % TODO: turn these into hard brackets?
\text{\textcolor[rgb]{.3,.6,.1}{\$tart}} 	& 0 & 0 & 0 & 0 & \ldots & 0 & 0\\
\text{I} 		& 3 & 0 & 0 & 2 & \ldots & 0 & 0\\
\text{am} 		& 0 & 1 & 0 & 0 & \ldots & 0 & 0\\
\text{Sam} 		& 0 & 0 & 1 & 1 & \ldots & 0 & 0\\
& \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
\text{ham.} 	& 0 & 0 & 0 & 0 & \ldots & 0 & 0\\
\text{\textcolor[rgb]{1,0,0}{\$top}} 		& 0 & 0 & 0 & 0 & \ldots & 1 & 1\\
\end{block}\end{blockarray}
\end{align*}
Now we divide each column by the column sum so that each column sums to one.

\begin{align*}
\begin{blockarray}{cccccccc}
& \text{\textcolor[rgb]{.3,.6,.1}{\$tart}} & \text{I} & \text{am} & \text{Sam} & & \text{ham.} & \text{\textcolor[rgb]{1,0,0}{\$top}} \\
\begin{block}{c(ccccccc)} % TODO: turn these into hard brackets?
\text{\textcolor[rgb]{.3,.6,.1}{\$tart}} & 0 & 0 & 0 & 0 & \ldots & 0 & 0\\
\text{I} 		& 3/4 & 0 & 0 & 2/3 & \ldots & 0 & 0\\
\text{am} 		& 0 & 1/5 & 0 & 0 & \ldots & 0 & 0\\
\text{Sam} 		& 0 & 0 & 1 & 1/3 & \ldots & 0 & 0\\
& \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
\text{ham.} 	& 0 & 0 & 0 & 0 & \ldots & 0 & 0\\
\text{\textcolor[rgb]{1,0,0}{\$top}} 	& 0 & 0 & 0 & 0 & \ldots & 1 & 1\\
\end{block}\end{blockarray}
\end{align*}

The $3/4$ correctly indicates that 3 out of 4 times, the sentences in the training set start with the word ``I''.
Similarly, the $2/3$ and $1/3$ tell us that ``Sam'' is followed by ``I'' twice and by ``Sam'' once in the training set.
Note that ``am'' (without a period) always transitions to ``Sam'' and that ``ham.'' (with a period) always transitions the stop state.
Finally, to avoid a column of zeros, we place a one in the bottom right hand corner of the matrix (the end state always transitions to itself).

The entire procedure of creating the transition matrix for the Markov chain with words from a file as states, with some hints for implementation, is summarized below in Algorithm \ref{alg:MarkovSentencesTransitionMatrix}.

\newpage % Might be unnecessary in later editions.

\begin{algorithm} % Read a file and convert it into a Markov chain.
\begin{algorithmic}[1]
\Procedure{MakeTransitionMatrix}{}
\State Count the number of unique words in the training set.
\State Initialize a square array of zeros of the appropriate size to be the transition matrix (remember to account for the start and stop states).
\State Initialize a list of states, beginning with ``\$tart''.
\For {each sentence in the training set}
    \State Split the sentence into a list of words.
    \State Add each \emph{new} word to the list of states.
    \State Add 1 to the entry of the transition matrix corresponding to the start state transitioning to the first word of the sentence.
   	\State For each consecutive pair $(a, b)$ of words in the list of words, add 1 to the entry of the transition matrix corresponding to moving from state $a$ to state $b$.
	\item Add 1 to the entry of the transition matrix corresponding to the last word of the sentence transitioning to the stop state.
\EndFor
\State Make sure the stop state transitions to itself.
\State Normalize each column by dividing by the column sums (hint: array broadcasting).
\EndProcedure
\end{algorithmic}
\caption{Convert a training set of sentences into a Markov chain.}
\label{alg:MarkovSentencesTransitionMatrix}
\end{algorithm}

\begin{problem} % Problem: Class that makes a Markov chain from a file.
Write a class called \li{SentenceGenerator}.
The constructor should accept a filename (for the training set).
Read the file and build a transition matrix from its contents.
You may assume that the file has one complete sentence written on each line.
\label{problem:MarkovClassPt1}
\end{problem}

% TODO: Put a picture of the actual Markov chain here and discuss a little.

\begin{problem} % Problem: Create random sentences
Add a method to the \li{SentenceGenerator} class called \li{babble()}.
Begin at the start state and use the strategy from Problem \ref{problem:transition} to transition through the object's Markov chain.
Keep track of the path through the chain and the corresponding path of words.
When the stop state is reached, stop transitioning and terminate the sentence.
Return the resulting sentence as a single string.

For example, your \li{SentenceGenerator} class should be able to create random sentences that sound somewhat like Yoda speaking.
\begin{lstlisting}
>>> yoda = SentenceGenerator("Yoda.txt")
>>> for i in xrange(5):
... 	print(yoda.babble())
...
<<
Impossible to my size, do not!
For eight hundred years old to enter the dark side of Congress there is.
But beware of the Wookiees, I have.
Fear leads to eat as well.
But agree on this, we must, and find your weapon!>>
\end{lstlisting}
\end{problem}

\section*{Additional Material} % ==============================================

\subsection*{Large Training Sets} % -------------------------------------------

The approach in the previous problems begins to fail as the training set grows larger.
For example, a single Shakespearean play may not be large enough to cause memory problems, but \emph{The Complete Works of William Shakespeare} certainly will.

To accommodate larger data sets, consider use a sparse matrix for the transition matrix in instead of a regular NumPy array. %(use the \li{lil_matrix} from the \li{scipy.sparse} library). % Why lil_matrix?
Ensure that the process still works on small training sets, then proceed to larger training sets.
How are the resulting sentences different if a very large training set is used instead of a small training set?

% \subsection*{Natural Language Processing Tools} % -----------------------------

% TODO: A brief overview of the \li{nltk} package.
